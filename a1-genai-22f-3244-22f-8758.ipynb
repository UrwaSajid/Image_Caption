{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329},{"sourceId":14810356,"sourceType":"datasetVersion","datasetId":9470586}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Feature Extraction**","metadata":{}},{"cell_type":"code","source":"# import os, pickle, torch, torch.nn as nn\n# from torchvision import models, transforms\n# from torch.utils.data import DataLoader, Dataset\n# from PIL import Image\n# from tqdm import tqdm\n\n# def find_image_dir():\n#     # Common Kaggle root\n#     base_input = '/kaggle/input/datasets/adityajn105/flickr30k'\n#     # Walk through the input directory to find where the images actually are\n#     for root, dirs, files in os.walk(base_input):\n#         # Look for the folder containing a high volume of jpg files\n#         if len([f for f in files if f.endswith('.jpg')]) > 1000:\n#             return root\n#     return None\n\n# IMAGE_DIR = find_image_dir()\n# OUTPUT_FILE = 'flickr30k_features.pkl'\n\n# if IMAGE_DIR:\n#     print(f\"✓ Found images at: {IMAGE_DIR}\")\n# else:\n#     raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n\n# # --- THE DATASET CLASS ---\n# class FlickrDataset(Dataset):\n#     def __init__(self, img_dir, transform):\n#         self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n#         self.transform = transform\n#         self.img_dir = img_dir\n    \n#     def __len__(self):\n#         return len(self.img_names)\n    \n#     def __getitem__(self, idx):\n#         name = self.img_names[idx]\n#         img_path = os.path.join(self.img_dir, name)\n#         img = Image.open(img_path).convert('RGB')\n#         return self.transform(img), name\n\n# # --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n# model = nn.Sequential(*list(model.children())[:-1])  # Feature vector only\n# model = nn.DataParallel(model).to(device)\n# model.eval()\n\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n# ])\n\n# dataset = FlickrDataset(IMAGE_DIR, transform)\n# loader = DataLoader(dataset, batch_size=128, num_workers=4)\n\n# features_dict = {}\n# with torch.no_grad():\n#     for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n#         feats = model(imgs.to(device))\n#         feats = feats.squeeze(-1).squeeze(-1)\n#         for i, name in enumerate(names):\n#             features_dict[name] = feats[i].cpu().numpy().astype('float32')\n\n# with open(OUTPUT_FILE, 'wb') as f:\n#     pickle.dump(features_dict, f)\n\n# print(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-10T17:32:30.178544Z","iopub.execute_input":"2026-02-10T17:32:30.179394Z","iopub.status.idle":"2026-02-10T17:34:48.371757Z","shell.execute_reply.started":"2026-02-10T17:32:30.179361Z","shell.execute_reply":"2026-02-10T17:34:48.370931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Normalization and Tokenization**","metadata":{}},{"cell_type":"code","source":"# import csv\n# import pickle\n# import re\n# from collections import Counter\n\n# # --- LOAD CAPTIONS ---\n# CAPTIONS_FILE = '/kaggle/input/datasets/adityajn105/flickr30k/captions.txt'\n\n# def load_captions(file_path):\n#     \"\"\"Load captions from file and organize by image\"\"\"\n#     captions_dict = {}\n    \n#     with open(file_path, 'r', encoding='utf-8', newline='') as f:\n#         reader = csv.reader(f)\n#         next(reader, None)  # Skip header line\n#         for row in reader:\n#             if not row or len(row) < 2:\n#                 continue\n            \n#             img_caption = row[0].strip()\n#             caption = ','.join(row[1:]).strip()  # Preserve commas in captions\n#             if not img_caption or not caption:\n#                 continue\n            \n#             # Extract image name (remove caption number like #0, #1, etc.)\n#             img_name = img_caption.rsplit('#', 1)[0]\n            \n#             if img_name not in captions_dict:\n#                 captions_dict[img_name] = []\n#             captions_dict[img_name].append(caption)\n    \n#     return captions_dict\n\n# # --- NORMALIZATION ---\n# def normalize_caption(caption):\n#     \"\"\"Normalize caption text\"\"\"\n#     if not caption:\n#         return ''\n    \n#     # Convert to lowercase\n#     caption = caption.lower()\n    \n#     # Remove special characters and extra spaces\n#     caption = re.sub(r'[^a-z0-9\\s]', '', caption)\n    \n#     # Remove multiple spaces\n#     caption = re.sub(r'\\s+', ' ', caption)\n    \n#     return caption.strip()\n\n# # --- TOKENIZATION ---\n# def tokenize_caption(caption):\n#     \"\"\"Word-level tokenization\"\"\"\n#     return caption.split()\n\n# # --- BUILD VOCABULARY ---\n# def build_vocabulary(captions_dict, min_freq=5):\n#     \"\"\"Build vocabulary from all captions\"\"\"\n#     word_counter = Counter()\n    \n#     # Count all words\n#     for img_name, captions in captions_dict.items():\n#         for caption in captions:\n#             normalized = normalize_caption(caption)\n#             if not normalized:\n#                 continue\n#             tokens = tokenize_caption(normalized)\n#             word_counter.update(tokens)\n    \n#     # Filter by minimum frequency\n#     vocab = [word for word, freq in word_counter.items() if freq >= min_freq]\n    \n#     # Sort vocabulary for consistency\n#     vocab = sorted(vocab)\n    \n#     return vocab, word_counter\n\n# # --- CREATE TOKEN MAPPINGS ---\n# def create_token_mappings(vocab):\n#     \"\"\"Create word-to-index and index-to-word mappings\"\"\"\n#     # Special tokens\n#     SPECIAL_TOKENS = {\n#         '<PAD>': 0,\n#         '<START>': 1,\n#         '<END>': 2,\n#         '<UNK>': 3  # Unknown token for words not in vocabulary\n#     }\n    \n#     # Start regular vocabulary after special tokens\n#     word2idx = SPECIAL_TOKENS.copy()\n#     idx2word = {idx: word for word, idx in SPECIAL_TOKENS.items()}\n    \n#     # Add vocabulary words\n#     for idx, word in enumerate(vocab, start=len(SPECIAL_TOKENS)):\n#         word2idx[word] = idx\n#         idx2word[idx] = word\n    \n#     return word2idx, idx2word\n\n# # --- ENCODE CAPTIONS ---\n# def encode_caption(caption, word2idx, add_special_tokens=True):\n#     \"\"\"Convert caption to token IDs\"\"\"\n#     normalized = normalize_caption(caption)\n#     tokens = tokenize_caption(normalized) if normalized else []\n    \n#     # Convert words to indices\n#     token_ids = []\n    \n#     if add_special_tokens:\n#         token_ids.append(word2idx['<START>'])\n    \n#     for token in tokens:\n#         # Use <UNK> token for words not in vocabulary\n#         token_ids.append(word2idx.get(token, word2idx['<UNK>']))\n    \n#     if add_special_tokens:\n#         token_ids.append(word2idx['<END>'])\n    \n#     return token_ids\n\n# # --- MAIN PROCESSING ---\n# print(\"Loading captions...\")\n# captions_dict = load_captions(CAPTIONS_FILE)\n# print(f\"Loaded captions for {len(captions_dict)} images\")\n\n# print(\"\\nBuilding vocabulary...\")\n# vocab, word_counter = build_vocabulary(captions_dict, min_freq=5)\n# print(f\"Vocabulary size: {len(vocab)} words (min frequency: 5)\")\n# print(f\"Total unique words before filtering: {len(word_counter)}\")\n\n# print(\"\\nCreating token mappings...\")\n# word2idx, idx2word = create_token_mappings(vocab)\n# print(f\"Total vocabulary size (including special tokens): {len(word2idx)}\")\n\n# # --- ENCODE ALL CAPTIONS ---\n# print(\"\\nEncoding all captions...\")\n# encoded_captions = {}\n# for img_name, captions in captions_dict.items():\n#     encoded_captions[img_name] = []\n#     for caption in captions:\n#         token_ids = encode_caption(caption, word2idx, add_special_tokens=True)\n#         encoded_captions[img_name].append(token_ids)\n\n# # --- SAVE EVERYTHING ---\n# output_data = {\n#     'captions_dict': captions_dict,  # Original captions\n#     'encoded_captions': encoded_captions,  # Tokenized captions\n#     'word2idx': word2idx,  # Word to index mapping\n#     'idx2word': idx2word,  # Index to word mapping\n#     'vocab': vocab,  # Vocabulary list\n#     'word_counter': word_counter  # Word frequencies\n# }\n\n# OUTPUT_FILE = 'flickr30k_captions_processed.pkl'\n# with open(OUTPUT_FILE, 'wb') as f:\n#     pickle.dump(output_data, f)\n\n# print(f\"\\n✓ Success! Saved to {OUTPUT_FILE}\")\n\n# # --- PRINT STATISTICS ---\n# print(\"\\n\" + \"=\"*60)\n# print(\"STATISTICS\")\n# print(\"=\"*60)\n# print(f\"Special Tokens:\")\n# print(f\"  <PAD>: {word2idx['<PAD>']}\")\n# print(f\"  <START>: {word2idx['<START>']}\")\n# print(f\"  <END>: {word2idx['<END>']}\")\n# print(f\"  <UNK>: {word2idx['<UNK>']}\")\n# print(f\"\\nVocabulary size: {len(word2idx)}\")\n# print(f\"Number of images: {len(captions_dict)}\")\n# print(f\"Total captions: {sum(len(caps) for caps in captions_dict.values())}\")\n\n# # Calculate average caption length\n# all_lengths = []\n# for caps in encoded_captions.values():\n#     for cap in caps:\n#         all_lengths.append(len(cap))\n# print(f\"Average caption length (with special tokens): {sum(all_lengths) / len(all_lengths):.2f}\")\n\n# # --- SHOW EXAMPLE ---\n# print(\"\\n\" + \"=\"*60)\n# print(\"EXAMPLE\")\n# print(\"=\"*60)\n# sample_img = list(captions_dict.keys())[0]\n# print(f\"Image: {sample_img}\")\n# print(f\"\\nOriginal caption:\")\n# print(f\"  {captions_dict[sample_img][0]}\")\n# print(f\"\\nNormalized caption:\")\n# normalized = normalize_caption(captions_dict[sample_img][0])\n# print(f\"  {normalized}\")\n# print(f\"\\nTokenized:\")\n# print(f\"  {tokenize_caption(normalized)}\")\n# print(f\"\\nEncoded (token IDs):\")\n# print(f\"  {encoded_captions[sample_img][0]}\")\n# print(f\"\\nDecoded back:\")\n# decoded = [idx2word[idx] for idx in encoded_captions[sample_img][0]]\n# print(f\"  {decoded}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T20:04:33.013114Z","iopub.execute_input":"2026-02-10T20:04:33.013325Z","iopub.status.idle":"2026-02-10T20:04:36.554562Z","shell.execute_reply.started":"2026-02-10T20:04:33.013304Z","shell.execute_reply":"2026-02-10T20:04:36.553839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Code LSTM**","metadata":{}},{"cell_type":"code","source":"# import pickle\n# import random\n# import numpy as np\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torch.nn.functional as F\n# from torch.utils.data import Dataset, DataLoader\n# from torch.nn.utils.rnn import pad_sequence\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n\n# # Set random seeds for reproducibility\n# torch.manual_seed(42)\n# np.random.seed(42)\n# random.seed(42)\n\n# # ============================================================================\n# # CONFIGURATION\n# # ============================================================================\n# class Config:\n#     # Paths\n#     FEATURES_PATH = '/kaggle/input/genai-a1-22f3244-22f8758/archive (1)/flickr30k_features.pkl'\n#     CAPTIONS_PATH = '/kaggle/input/genai-a1-22f3244-22f8758/archive (1)/flickr30k_captions_processed.pkl'\n    \n#     # Model Architecture\n#     IMAGE_FEATURE_DIM = 2048\n#     HIDDEN_DIM = 512\n#     ENCODER_LAYERS = 2\n#     DECODER_LAYERS = 2\n#     EMBEDDING_DIM = 512\n#     DROPOUT = 0.5\n    \n#     # Training\n#     BATCH_SIZE = 64\n#     LEARNING_RATE = 0.0005\n#     NUM_EPOCHS = 30\n#     TEACHER_FORCING_RATIO_START = 0.9\n#     TEACHER_FORCING_RATIO_END = 0.5\n#     MAX_CAPTION_LENGTH = 50\n#     LABEL_SMOOTHING = 0.1\n    \n#     # Learning Rate Scheduling\n#     LR_PATIENCE = 2\n#     LR_FACTOR = 0.5\n    \n#     # Beam Search\n#     BEAM_WIDTH = 5\n    \n#     # Data Split\n#     TRAIN_SPLIT = 0.8\n#     VAL_SPLIT = 0.1\n    \n#     # Device\n#     DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# config = Config()\n\n# # ============================================================================\n# # LOAD DATA\n# # ============================================================================\n# print(\"Loading data...\")\n# with open(config.FEATURES_PATH, 'rb') as f:\n#     features_dict = pickle.load(f)\n\n# with open(config.CAPTIONS_PATH, 'rb') as f:\n#     captions_data = pickle.load(f)\n\n# encoded_captions = captions_data['encoded_captions']\n# word2idx = captions_data['word2idx']\n# idx2word = captions_data['idx2word']\n# vocab_size = len(word2idx)\n# pad_idx = word2idx['<PAD>']\n\n# common_images = set(features_dict.keys()) & set(encoded_captions.keys())\n# if not common_images:\n#     raise ValueError(\"No overlap between image features and captions. Check filenames and paths.\")\n# sample_feature = next(iter(features_dict.values()))\n# if sample_feature.shape[0] != config.IMAGE_FEATURE_DIM:\n#     raise ValueError(\n#         f\"Feature dim mismatch: expected {config.IMAGE_FEATURE_DIM}, got {sample_feature.shape[0]}\"\n#     )\n# print(f\"✓ Loaded {len(features_dict)} image features\")\n# print(f\"✓ Loaded captions for {len(encoded_captions)} images\")\n# print(f\"✓ Overlap images: {len(common_images)}\")\n# print(f\"✓ Vocabulary size: {vocab_size}\")\n# print(f\"✓ Device: {config.DEVICE}\")\n\n# # ============================================================================\n# # DATASET CLASS\n# # ============================================================================\n# class FlickrDataset(Dataset):\n#     def __init__(self, image_names, features_dict, encoded_captions, max_length):\n#         self.data = []\n        \n#         for img_name in image_names:\n#             if img_name in features_dict and img_name in encoded_captions:\n#                 feature = features_dict[img_name]\n#                 for caption in encoded_captions[img_name]:\n#                     if len(caption) <= max_length:\n#                         self.data.append((feature, caption, img_name))\n    \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, idx):\n#         feature, caption, img_name = self.data[idx]\n#         return torch.FloatTensor(feature), torch.LongTensor(caption), img_name\n\n# def collate_fn(batch):\n#     features, captions, img_names = zip(*batch)\n#     features = torch.stack(features)\n#     captions_padded = pad_sequence(captions, batch_first=True, padding_value=pad_idx)\n#     return features, captions_padded, img_names\n\n# # ============================================================================\n# # SPLIT DATA\n# # ============================================================================\n# all_image_names = list(common_images)\n# random.shuffle(all_image_names)\n\n# n_total = len(all_image_names)\n# n_train = int(n_total * config.TRAIN_SPLIT)\n# n_val = int(n_total * config.VAL_SPLIT)\n\n# train_names = all_image_names[:n_train]\n# val_names = all_image_names[n_train:n_train + n_val]\n# test_names = all_image_names[n_train + n_val:]\n\n# print(f\"\\nData Split:\")\n# print(f\"  Train: {len(train_names)} images\")\n# print(f\"  Val: {len(val_names)} images\")\n# print(f\"  Test: {len(test_names)} images\")\n\n# train_dataset = FlickrDataset(train_names, features_dict, encoded_captions, config.MAX_CAPTION_LENGTH)\n# val_dataset = FlickrDataset(val_names, features_dict, encoded_captions, config.MAX_CAPTION_LENGTH)\n# test_dataset = FlickrDataset(test_names, features_dict, encoded_captions, config.MAX_CAPTION_LENGTH)\n\n# train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n# val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n# test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# print(f\"  Train batches: {len(train_loader)}\")\n# print(f\"  Val batches: {len(val_loader)}\")\n# print(f\"  Test batches: {len(test_loader)}\")\n\n# # ============================================================================\n# # LSTM ENCODER (FROM SCRATCH VIA LSTMCell)\n# # ============================================================================\n# class LSTMEncoder(nn.Module):\n#     def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n#         super(LSTMEncoder, self).__init__()\n#         self.hidden_dim = hidden_dim\n#         self.num_layers = num_layers\n        \n#         self.feature_projection = nn.Sequential(\n#             nn.Linear(input_dim, hidden_dim),\n#             nn.LayerNorm(hidden_dim),\n#             nn.ReLU(),\n#             nn.Dropout(dropout)\n#         )\n        \n#         self.h_init = nn.Linear(hidden_dim, hidden_dim)\n#         self.c_init = nn.Linear(hidden_dim, hidden_dim)\n        \n#         self.dropout = nn.Dropout(dropout)\n    \n#     def forward(self, image_features):\n#         x = self.feature_projection(image_features)\n#         h0 = torch.tanh(self.h_init(x))\n#         c0 = torch.tanh(self.c_init(x))\n        \n#         h_states = []\n#         c_states = []\n#         for _ in range(self.num_layers):\n#             h_states.append(self.dropout(h0))\n#             c_states.append(c0)\n        \n#         h_states = torch.stack(h_states)\n#         c_states = torch.stack(c_states)\n#         return h_states, c_states\n\n# # ============================================================================\n# # LSTM DECODER (FROM SCRATCH VIA LSTMCell)\n# # ============================================================================\n# class LSTMDecoder(nn.Module):\n#     def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n#         super(LSTMDecoder, self).__init__()\n#         self.hidden_dim = hidden_dim\n#         self.num_layers = num_layers\n#         self.vocab_size = vocab_size\n        \n#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n#         self.lstm_cells = nn.ModuleList([\n#             nn.LSTMCell(embedding_dim if i == 0 else hidden_dim, hidden_dim)\n#             for i in range(num_layers)\n#         ])\n#         self.layer_norms = nn.ModuleList([\n#             nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n#         ])\n#         self.dropout = nn.Dropout(dropout)\n#         self.fc_out = nn.Linear(hidden_dim, vocab_size)\n    \n#     def forward(self, input_token, hidden_states, cell_states):\n#         embedded = self.embedding(input_token)\n#         embedded = self.dropout(embedded)\n        \n#         x = embedded\n#         new_h_states = []\n#         new_c_states = []\n        \n#         for layer_idx in range(self.num_layers):\n#             h_prev = hidden_states[layer_idx]\n#             c_prev = cell_states[layer_idx]\n            \n#             h, c = self.lstm_cells[layer_idx](x, (h_prev, c_prev))\n#             h = self.layer_norms[layer_idx](h)\n            \n#             new_h_states.append(h)\n#             new_c_states.append(c)\n#             x = self.dropout(h)\n        \n#         output = self.fc_out(x)\n#         new_h_states = torch.stack(new_h_states)\n#         new_c_states = torch.stack(new_c_states)\n#         return output, new_h_states, new_c_states\n\n# # ============================================================================\n# # SEQ2SEQ MODEL WITHOUT ATTENTION\n# # ============================================================================\n# class Seq2SeqImageCaptioning(nn.Module):\n#     def __init__(self, encoder, decoder, device):\n#         super(Seq2SeqImageCaptioning, self).__init__()\n#         self.encoder = encoder\n#         self.decoder = decoder\n#         self.device = device\n    \n#     def forward(self, image_features, captions, teacher_forcing_ratio=0.5):\n#         batch_size = image_features.size(0)\n#         max_length = captions.size(1)\n        \n#         hidden_states, cell_states = self.encoder(image_features)\n#         outputs = torch.zeros(batch_size, max_length, self.decoder.vocab_size).to(self.device)\n#         input_token = captions[:, 0]\n        \n#         for t in range(1, max_length):\n#             output, hidden_states, cell_states = self.decoder(\n#                 input_token, hidden_states, cell_states\n#             )\n#             outputs[:, t, :] = output\n            \n#             teacher_force = random.random() < teacher_forcing_ratio\n#             top1 = output.argmax(1)\n#             input_token = captions[:, t] if teacher_force else top1\n        \n#         return outputs\n\n# # ============================================================================\n# # INITIALIZE MODEL\n# # ============================================================================\n# encoder = LSTMEncoder(\n#     input_dim=config.IMAGE_FEATURE_DIM,\n#     hidden_dim=config.HIDDEN_DIM,\n#     num_layers=config.ENCODER_LAYERS,\n#     dropout=config.DROPOUT\n#  )\n\n# decoder = LSTMDecoder(\n#     vocab_size=vocab_size,\n#     embedding_dim=config.EMBEDDING_DIM,\n#     hidden_dim=config.HIDDEN_DIM,\n#     num_layers=config.DECODER_LAYERS,\n#     dropout=config.DROPOUT\n#  )\n\n# model = Seq2SeqImageCaptioning(encoder, decoder, config.DEVICE).to(config.DEVICE)\n\n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# print(f\"\\n✓ Model initialized with {count_parameters(model):,} trainable parameters\")\n\n# # ============================================================================\n# # LOSS AND OPTIMIZER\n# # ============================================================================\n# criterion = nn.CrossEntropyLoss(\n#     ignore_index=pad_idx,\n#     label_smoothing=config.LABEL_SMOOTHING\n#  )\n\n# optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n#     optimizer,\n#     mode='min',\n#     factor=config.LR_FACTOR,\n#     patience=config.LR_PATIENCE\n#  )\n\n# # ============================================================================\n# # TRAINING FUNCTIONS\n# # ============================================================================\n# def get_teacher_forcing_ratio(epoch, start_ratio, end_ratio, num_epochs):\n#     return end_ratio + (start_ratio - end_ratio) * (1 - epoch / num_epochs)\n\n# def compute_accuracy(logits, targets, pad_token):\n#     preds = logits.argmax(dim=-1)\n#     mask = targets != pad_token\n#     correct = (preds == targets) & mask\n#     total = mask.sum().item()\n#     correct_count = correct.sum().item()\n#     return correct_count, total\n\n# def train_epoch(model, loader, optimizer, criterion, device, teacher_forcing_ratio):\n#     model.train()\n#     epoch_loss = 0\n#     correct_total = 0\n#     token_total = 0\n    \n#     for features, captions, _ in tqdm(loader, desc=\"Training\", leave=False):\n#         features = features.to(device)\n#         captions = captions.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = model(features, captions, teacher_forcing_ratio)\n#         logits = outputs[:, 1:, :]\n#         targets = captions[:, 1:]\n        \n#         loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n#         loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n#         optimizer.step()\n#         epoch_loss += loss.item()\n        \n#         correct, total = compute_accuracy(logits, targets, pad_idx)\n#         correct_total += correct\n#         token_total += total\n    \n#     avg_loss = epoch_loss / len(loader)\n#     avg_acc = correct_total / max(token_total, 1)\n#     return avg_loss, avg_acc\n\n# def evaluate(model, loader, criterion, device):\n#     model.eval()\n#     epoch_loss = 0\n#     correct_total = 0\n#     token_total = 0\n    \n#     with torch.no_grad():\n#         for features, captions, _ in tqdm(loader, desc=\"Evaluating\", leave=False):\n#             features = features.to(device)\n#             captions = captions.to(device)\n#             outputs = model(features, captions, teacher_forcing_ratio=1.0)\n#             logits = outputs[:, 1:, :]\n#             targets = captions[:, 1:]\n#             loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n#             epoch_loss += loss.item()\n            \n#             correct, total = compute_accuracy(logits, targets, pad_idx)\n#             correct_total += correct\n#             token_total += total\n    \n#     avg_loss = epoch_loss / len(loader)\n#     avg_acc = correct_total / max(token_total, 1)\n#     return avg_loss, avg_acc\n\n# # ============================================================================\n# # TRAINING LOOP\n# # ============================================================================\n# print(\"\\n\" + \"=\"*60)\n# print(\"TRAINING\")\n# print(\"=\"*60)\n\n# train_losses = []\n# val_losses = []\n# train_accs = []\n# val_accs = []\n# learning_rates = []\n# best_val_loss = float('inf')\n\n# for epoch in range(config.NUM_EPOCHS):\n#     teacher_forcing_ratio = get_teacher_forcing_ratio(\n#         epoch,\n#         config.TEACHER_FORCING_RATIO_START,\n#         config.TEACHER_FORCING_RATIO_END,\n#         config.NUM_EPOCHS\n#     )\n    \n#     print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS} (TF Ratio: {teacher_forcing_ratio:.3f})\")\n#     train_loss, train_acc = train_epoch(\n#         model, train_loader, optimizer, criterion,\n#         config.DEVICE, teacher_forcing_ratio\n#     )\n#     val_loss, val_acc = evaluate(model, val_loader, criterion, config.DEVICE)\n    \n#     train_losses.append(train_loss)\n#     val_losses.append(val_loss)\n#     train_accs.append(train_acc)\n#     val_accs.append(val_acc)\n#     learning_rates.append(optimizer.param_groups[0]['lr'])\n    \n#     print(f\"  Train Loss: {train_loss:.4f}\")\n#     print(f\"  Val Loss: {val_loss:.4f}\")\n#     print(f\"  Train Acc: {train_acc:.4f}\")\n#     print(f\"  Val Acc: {val_acc:.4f}\")\n#     print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n#     scheduler.step(val_loss)\n    \n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss\n#         torch.save(model.state_dict(), 'best_model_improved.pth')\n#         print(\"  ✓ Best model saved!\")\n\n# # ============================================================================\n# # PLOT LOSSES AND LEARNING RATE\n# # ============================================================================\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n# ax1.plot(range(1, config.NUM_EPOCHS + 1), train_losses, label='Training Loss', marker='o')\n# ax1.plot(range(1, config.NUM_EPOCHS + 1), val_losses, label='Validation Loss', marker='s')\n# ax1.set_xlabel('Epoch')\n# ax1.set_ylabel('Loss')\n# ax1.set_title('Training and Validation Loss Over Epochs')\n# ax1.legend()\n# ax1.grid(True)\n\n# ax2.plot(range(1, config.NUM_EPOCHS + 1), learning_rates, label='Learning Rate', marker='o', color='green')\n# ax2.set_xlabel('Epoch')\n# ax2.set_ylabel('Learning Rate')\n# ax2.set_title('Learning Rate Schedule')\n# ax2.legend()\n# ax2.grid(True)\n\n# plt.tight_layout()\n# plt.savefig('training_improved.png', dpi=300, bbox_inches='tight')\n# plt.show()\n\n# print(\"\\n✓ Training complete!\")\n\n# # ============================================================================\n# # BEAM SEARCH CAPTION GENERATION\n# # ============================================================================\n# def beam_search(model, image_feature, word2idx, idx2word, beam_width=5, max_length=50, device='cpu'):\n#     model.eval()\n    \n#     with torch.no_grad():\n#         image_feature = image_feature.unsqueeze(0).to(device)\n#         hidden_states, cell_states = model.encoder(image_feature)\n        \n#         start_token = word2idx['<START>']\n#         beams = [([start_token], 0.0, hidden_states, cell_states)]\n#         completed_sequences = []\n        \n#         for _ in range(max_length):\n#             candidates = []\n            \n#             for seq, score, h_states, c_states in beams:\n#                 if seq[-1] == word2idx['<END>']:\n#                     completed_sequences.append((seq, score))\n#                     continue\n                \n#                 input_token = torch.LongTensor([seq[-1]]).to(device)\n#                 output, new_h, new_c = model.decoder(input_token, h_states, c_states)\n                \n#                 log_probs = F.log_softmax(output, dim=-1)\n#                 top_log_probs, top_indices = log_probs.topk(beam_width, dim=-1)\n                \n#                 for i in range(beam_width):\n#                     token = top_indices[0, i].item()\n#                     token_score = top_log_probs[0, i].item()\n#                     new_seq = seq + [token]\n#                     new_score = score + token_score\n#                     candidates.append((new_seq, new_score, new_h, new_c))\n            \n#             if not candidates:\n#                 break\n            \n#             candidates.sort(key=lambda x: x[1], reverse=True)\n#             beams = candidates[:beam_width]\n            \n#             if all(seq[-1] == word2idx['<END>'] for seq, _, _, _ in beams):\n#                 break\n        \n#         for seq, score, _, _ in beams:\n#             if seq[-1] != word2idx['<END>']:\n#                 completed_sequences.append((seq, score))\n        \n#         if not completed_sequences:\n#             completed_sequences = [(seq, score) for seq, score, _, _ in beams]\n        \n#         best_seq, _ = max(completed_sequences, key=lambda x: x[1] / len(x[0]))\n#         caption = []\n#         for token_id in best_seq[1:]:\n#             if token_id == word2idx['<END>']:\n#                 break\n#             if token_id != word2idx['<PAD>']:\n#                 caption.append(idx2word[token_id])\n        \n#         return ' '.join(caption)\n\n# # ============================================================================\n# # EVALUATION METRICS\n# # ============================================================================\n# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# def calculate_bleu(references, hypothesis):\n#     smoothing = SmoothingFunction().method1\n#     return sentence_bleu(\n#         references,\n#         hypothesis,\n#         weights=(0.25, 0.25, 0.25, 0.25),\n#         smoothing_function=smoothing\n#     )\n\n# def calculate_token_metrics(references, hypothesis):\n#     ref_tokens = set()\n#     for ref in references:\n#         ref_tokens.update(ref)\n    \n#     hyp_tokens = set(hypothesis)\n#     tp = len(ref_tokens & hyp_tokens)\n#     fp = len(hyp_tokens - ref_tokens)\n#     fn = len(ref_tokens - hyp_tokens)\n    \n#     precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n#     recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n#     f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n#     return precision, recall, f1\n\n# # ============================================================================\n# # EVALUATION ON TEST SET\n# # ============================================================================\n# print(\"\\n\" + \"=\"*60)\n# print(\"EVALUATION ON TEST SET\")\n# print(\"=\"*60)\n# model.load_state_dict(torch.load('best_model_improved.pth'))\n# model.eval()\n\n# bleu_scores_beam = []\n# precisions = []\n# recalls = []\n# f1_scores = []\n# sample_predictions = []\n\n# print(\"\\nGenerating captions with Beam Search (this may take a while)...\")\n\n# with torch.no_grad():\n#     for features, captions, img_names in tqdm(test_loader, desc=\"Testing\"):\n#         features = features.to(config.DEVICE)\n        \n#         for i in range(features.size(0)):\n#             img_name = img_names[i]\n#             generated_beam = beam_search(\n#                 model, features[i].cpu(), word2idx, idx2word,\n#                 beam_width=config.BEAM_WIDTH,\n#                 max_length=config.MAX_CAPTION_LENGTH, device=config.DEVICE\n#             )\n            \n#             reference_captions = []\n#             for caption_ids in encoded_captions[img_name]:\n#                 tokens = [idx2word[idx] for idx in caption_ids\n#                          if idx not in [word2idx['<START>'], word2idx['<END>'], word2idx['<PAD>']]]\n#                 reference_captions.append(tokens)\n            \n#             generated_tokens_beam = generated_beam.split()\n#             bleu_beam = calculate_bleu(reference_captions, generated_tokens_beam)\n#             bleu_scores_beam.append(bleu_beam)\n            \n#             precision, recall, f1 = calculate_token_metrics(reference_captions, generated_tokens_beam)\n#             precisions.append(precision)\n#             recalls.append(recall)\n#             f1_scores.append(f1)\n            \n#             if len(sample_predictions) < 10:\n#                 sample_predictions.append({\n#                     'image': img_name,\n#                     'generated_beam': generated_beam,\n#                     'references': [' '.join(ref) for ref in reference_captions]\n#                 })\n\n# print(f\"\\n{'='*60}\")\n# print(\"QUANTITATIVE EVALUATION RESULTS\")\n# print(f\"{'='*60}\")\n# print(f\"\\nBeam Search (width={config.BEAM_WIDTH}):\")\n# print(f\"  BLEU-4 Score: {np.mean(bleu_scores_beam):.4f} (±{np.std(bleu_scores_beam):.4f})\")\n# print(f\"  Precision: {np.mean(precisions):.4f} (±{np.std(precisions):.4f})\")\n# print(f\"  Recall: {np.mean(recalls):.4f} (±{np.std(recalls):.4f})\")\n# print(f\"  F1-Score: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})\")\n\n# print(\"\\n\" + \"=\"*60)\n# print(\"SAMPLE PREDICTIONS\")\n# print(\"=\"*60)\n# for i, sample in enumerate(sample_predictions[:5], 1):\n#     print(f\"\\n{i}. Image: {sample['image']}\")\n#     print(f\"   Generated (Beam):   {sample['generated_beam']}\")\n#     print(f\"   References:\")\n#     for j, ref in enumerate(sample['references'], 1):\n#         print(f\"     {j}. {ref}\")\n\n# print(\"\\n✓ Evaluation complete!\")\n\n# results = {\n#     'train_losses': train_losses,\n#     'val_losses': val_losses,\n#     'train_accs': train_accs,\n#     'val_accs': val_accs,\n#     'learning_rates': learning_rates,\n#     'bleu_scores_beam': bleu_scores_beam,\n#     'precisions': precisions,\n#     'recalls': recalls,\n#     'f1_scores': f1_scores,\n#     'sample_predictions': sample_predictions,\n#     'config': {\n#         'hidden_dim': config.HIDDEN_DIM,\n#         'encoder_layers': config.ENCODER_LAYERS,\n#         'decoder_layers': config.DECODER_LAYERS,\n#         'beam_width': config.BEAM_WIDTH,\n#         'label_smoothing': config.LABEL_SMOOTHING\n#     }\n# }\n\n# with open('evaluation_results_improved.pkl', 'wb') as f:\n#     pickle.dump(results, f)\n\n# print(\"✓ Results saved to evaluation_results_improved.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T19:38:26.511143Z","iopub.execute_input":"2026-02-10T19:38:26.511439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inference and Gradio UI**","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport gradio as gr\n\n# =========================\n# CONFIGURATION\n# =========================\nclass Config:\n    FEATURES_PATH = \"/kaggle/input/genai-a1-22f3244-22f8758/archive (1)/flickr30k_features.pkl\"\n    CAPTIONS_PATH = \"/kaggle/input/genai-a1-22f3244-22f8758/archive (1)/flickr30k_captions_processed.pkl\"\n    MODEL_WEIGHTS = \"/kaggle/input/genai-a1-22f3244-22f8758/archive/best_model.pth\"\n\n    IMAGE_FEATURE_DIM = 2048\n    HIDDEN_DIM = 512\n    ENCODER_LAYERS = 2\n    DECODER_LAYERS = 2\n    EMBEDDING_DIM = 512\n    DROPOUT = 0.5\n\n    BEAM_WIDTH = 5\n    MAX_CAPTION_LENGTH = 50\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = Config()\n\n# =========================\n# LOAD TOKEN DATA\n# =========================\nwith open(config.CAPTIONS_PATH, \"rb\") as f:\n    captions_data = pickle.load(f)\n\nword2idx = captions_data[\"word2idx\"]\nidx2word = captions_data[\"idx2word\"]\nvocab_size = len(word2idx)\npad_idx = word2idx[\"<PAD>\"]\n\nfeatures_dict = {}\n\n# =========================\n# MODEL DEFINITION\n# =========================\nclass LSTMEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n        super(LSTMEncoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.feature_projection = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n\n        self.h_init = nn.Linear(hidden_dim, hidden_dim)\n        self.c_init = nn.Linear(hidden_dim, hidden_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, image_features):\n        x = self.feature_projection(image_features)\n        h0 = torch.tanh(self.h_init(x))\n        c0 = torch.tanh(self.c_init(x))\n\n        h_states = []\n        c_states = []\n        for _ in range(self.num_layers):\n            h_states.append(self.dropout(h0))\n            c_states.append(c0)\n\n        h_states = torch.stack(h_states)\n        c_states = torch.stack(c_states)\n        return h_states, c_states\n\nclass LSTMDecoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n        super(LSTMDecoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.vocab_size = vocab_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        self.lstm_cells = nn.ModuleList([\n            nn.LSTMCell(embedding_dim if i == 0 else hidden_dim, hidden_dim)\n            for i in range(num_layers)\n        ])\n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n        ])\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_token, hidden_states, cell_states):\n        embedded = self.embedding(input_token)\n        embedded = self.dropout(embedded)\n\n        x = embedded\n        new_h_states = []\n        new_c_states = []\n\n        for layer_idx in range(self.num_layers):\n            h_prev = hidden_states[layer_idx]\n            c_prev = cell_states[layer_idx]\n\n            h, c = self.lstm_cells[layer_idx](x, (h_prev, c_prev))\n            h = self.layer_norms[layer_idx](h)\n\n            new_h_states.append(h)\n            new_c_states.append(c)\n            x = self.dropout(h)\n\n        output = self.fc_out(x)\n        new_h_states = torch.stack(new_h_states)\n        new_c_states = torch.stack(new_c_states)\n        return output, new_h_states, new_c_states\n\nclass Seq2SeqImageCaptioning(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2SeqImageCaptioning, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n# =========================\n# LOAD MODEL\n# =========================\nencoder = LSTMEncoder(\n    input_dim=config.IMAGE_FEATURE_DIM,\n    hidden_dim=config.HIDDEN_DIM,\n    num_layers=config.ENCODER_LAYERS,\n    dropout=config.DROPOUT\n)\n\ndecoder = LSTMDecoder(\n    vocab_size=vocab_size,\n    embedding_dim=config.EMBEDDING_DIM,\n    hidden_dim=config.HIDDEN_DIM,\n    num_layers=config.DECODER_LAYERS,\n    dropout=config.DROPOUT\n)\n\nmodel = Seq2SeqImageCaptioning(encoder, decoder, config.DEVICE).to(config.DEVICE)\nstate_dict = torch.load(config.MODEL_WEIGHTS, map_location=config.DEVICE)\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n# =========================\n# CNN FEATURE EXTRACTOR\n# =========================\ncnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\ncnn = nn.Sequential(*list(cnn.children())[:-1]).to(config.DEVICE)\ncnn.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n# =========================\n# BEAM SEARCH\n# =========================\ndef beam_search(image_feature, beam_width=5, max_length=50):\n    with torch.no_grad():\n        image_feature = image_feature.unsqueeze(0).to(config.DEVICE)\n        hidden_states, cell_states = model.encoder(image_feature)\n\n        start_token = word2idx[\"<START>\"]\n        beams = [([start_token], 0.0, hidden_states, cell_states)]\n        completed_sequences = []\n\n        for _ in range(max_length):\n            candidates = []\n\n            for seq, score, h_states, c_states in beams:\n                if seq[-1] == word2idx[\"<END>\"]:\n                    completed_sequences.append((seq, score))\n                    continue\n\n                input_token = torch.LongTensor([seq[-1]]).to(config.DEVICE)\n                output, new_h, new_c = model.decoder(input_token, h_states, c_states)\n\n                log_probs = F.log_softmax(output, dim=-1)\n                top_log_probs, top_indices = log_probs.topk(beam_width, dim=-1)\n\n                for i in range(beam_width):\n                    token = top_indices[0, i].item()\n                    token_score = top_log_probs[0, i].item()\n                    new_seq = seq + [token]\n                    new_score = score + token_score\n                    candidates.append((new_seq, new_score, new_h, new_c))\n\n            if not candidates:\n                break\n\n            candidates.sort(key=lambda x: x[1], reverse=True)\n            beams = candidates[:beam_width]\n\n            if all(seq[-1] == word2idx[\"<END>\"] for seq, _, _, _ in beams):\n                break\n\n        for seq, score, _, _ in beams:\n            if seq[-1] != word2idx[\"<END>\"]:\n                completed_sequences.append((seq, score))\n\n        if not completed_sequences:\n            completed_sequences = [(seq, score) for seq, score, _, _ in beams]\n\n        best_seq, _ = max(completed_sequences, key=lambda x: x[1] / len(x[0]))\n\n        caption = []\n        for token_id in best_seq[1:]:\n            if token_id == word2idx[\"<END>\"]:\n                break\n            if token_id != word2idx[\"<PAD>\"]:\n                caption.append(idx2word[token_id])\n\n        return \" \".join(caption)\n\n# =========================\n# INFERENCE\n# =========================\ndef extract_feature_from_image(pil_image):\n    img_tensor = transform(pil_image).unsqueeze(0).to(config.DEVICE)\n    with torch.no_grad():\n        feats = cnn(img_tensor)\n        feats = feats.squeeze(-1).squeeze(-1)\n    return feats[0].cpu()\n\ndef generate_caption(image):\n    if image is None:\n        return \"No image provided.\"\n\n    feature = extract_feature_from_image(image)\n\n    caption = beam_search(feature, beam_width=config.BEAM_WIDTH, max_length=config.MAX_CAPTION_LENGTH)\n    return caption\n\n# =========================\n# GRADIO UI\n# =========================\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Flickr30k Image Captioning\")\n    gr.Markdown(\"Upload any image to generate a caption.\")\n    image_input = gr.Image(type=\"pil\", label=\"Input Image\")\n    output_text = gr.Textbox(label=\"Generated Caption\")\n    submit_btn = gr.Button(\"Generate\")\n\n    def _predict(image):\n        return generate_caption(image)\n\n    submit_btn.click(\n        _predict,\n        inputs=[image_input],\n        outputs=output_text\n    )\n\nif __name__ == \"__main__\":\n    demo.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:43:06.014137Z","iopub.execute_input":"2026-02-11T17:43:06.014410Z","iopub.status.idle":"2026-02-11T17:43:16.115848Z","shell.execute_reply.started":"2026-02-11T17:43:06.014376Z","shell.execute_reply":"2026-02-11T17:43:16.114882Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 97.8M/97.8M [00:00<00:00, 207MB/s] \n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://74a8359d23825e519a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://74a8359d23825e519a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":1}]}